# 1.머신러닝
  * 인공지능: 인공(Artificial) + 지능(Intelligence)
    * 개발자에 의한 인공지능
    * 데이터에 의한 인공지능
  * 머신러닝: 데이터를 기반으로 한 학습(learning)하는 기계(machine)
  * 딥러닝: 깊은(deep) 신경망 구조의 머신러닝

## 1-1.배경
  * 과거 컴퓨터로 데이터를 읽어들이고, 그 데이터 안에서 특징을 학습하여 패턴을 찾아내는 작업(패턴인식)
  * 데이터를 대량으로 수집 처리할 수 있는 환경이 갖춰짐으로 할수 있는 일들이 많아짐
  * 머신러닝은 데이터로부터 특징이나 패턴을 찾아내는 것이기 때문에 데이터가 가장 중요함
## 1-2. 정의
  * 인공지능의 한 분야로 컴퓨터가 학습할 수 있도록 하는 알고리즘과 기술을 개발하는 분야
  * "무엇(x)으로 무엇(y)을 예측하고 싶다"의 함수(f)를 찾아내는 것
  * y:출력변수(종속변수), x:d입력변수(독립변수), f:모형(머신러닝 알고리즘, 모델)

# 2.머신러닝으로 할 수 있는 것

## 2-1. 회귀(Regression)
  * 시계열(시간적인 변화를 연속적으로 관측한 데이터)데이터 같은 연속된 데이터를 취급할 때 사용하는 기법
      * 연속 데이터(Continuous data)는 연속적인 값을 가지는 데이터를 말합니다. 이는 숫자로 표현되는 데이터로, 무한한 가능한 값을 가질 수 있습니다.
  * 예) 과거 주식 추세를 학습해서 내일의 주가를 예측하는 시스템을 개발
 
## 2-2. 분류(Classification)
  * 주어진 데이터를 클래스별로 구별해 내는 과정으로 데이터와 데이터의 레이블값을 학습시키고 어느 범주에 속한 데이터인지 판단하고 예측
  * 예) 스팸메일인지 아닌지 구별해주는 시스템을 개발, 개 or 고양이 판별하기

## 2-3. 클러스터링(Clustering)
  * 분류와 비슷하지만 데이터에 레이블이 없음
  * 유사한 속성들을 갖는 데이터를 일정한 수의 군집으로 그룹핑하는 비지도 학습
  * 예) SNS 데이터를 통해 사회 이슈를 파악

# 3. 학습

## 3-1. 지도 학습(Supervised Learning)
  * 문제와 정답을 모두 학습시켜 예측 또는 분류하는 문제
  * y= f(x)에 대하여 입력 변수(x)와 출력 변수(y)의 관계예 대해 모델링 하는 것.

## 3-2. 비지도 학습(UnSupervised Learning)
  * 출력 변수(y)가 존재하지 않고, 입력 변수(x)간의 관계에 대해 모델링하는 것
  * 군집분석: 유사한 데이터끼리 그룹화
  * PCA: 독립변수들의 차원을 축소화

## 3-3. 자기지도 학습(Self-Supervised Learning)
  * 데이터 자체에서 스스로 레이블을 생성하여 학습에 이용하는 방법
  * 다량의 레이블이 없는 Raw Data로부터 데이터 부분들의 관계를 통해 Label을 자동으로 생성하여 지도 학습에 이용하는 비지도 학습 기법
  * BERT, GPT 모델

## 3-4. 강화 학습(Reinforcement Learning)
  * 수 많은 시뮬레이션을 통해 현재의 선택이 미래에 보상이 최대가 되도록 학습하는 방법
  * 레이블이 있는 데이터를 통해서 가중치와 편향을 학습하는 것과 비슷하게 보상이라는 개념을 사용하여 가중치와 편향을 학습하는 것
  * 결정을 순차적으로 내려야 하는 문제에 적용

# 4. 사이킷런(Scikit-Learn)모듈
  * 대표적인 파이썬 머신러닝 모듈
  * 다양한 머신러닝 알고리즘을 제공
  * 다양한 샘플 데이터를 제공
  * 머신러닝 결과를 검증하는 기능을 제공
  * BSD 라이선스이기 때문에 무료로 사용 및 배포가 가능
  * [사이킷런 공식 홈페이지](https://scikit-learn.org)

# 5. LinearSVC
  * 클래스를 구분으로 하는 분류 문제에서 각 클래스를 잘 구분하는 선을 그려주는 방식을 사용하는 알고리즘
  * 지도학습 알고리즘을 사용하는 학습 전용 데이터와 결과 전용 데이터를 함께 가지고 있어야 사용이 가능
  * ```python
    from sklearn.svm import LinearSVC
    from sklearn.metrics import accuracy_score # 머신러닝 결과를 검증하는 기능을 제공
    ```
  * 학습 데이터 준비
  * ```python
    learn_data =[[0,0],[1,0],[0,1],[1,1]] # 독립변수
    learn_label = [0,0,0,1] # 종속변수 (레이블이 4개)
    ```
  * 모델 객체 생성
  * ```python
    svc = LinearSVC()
    ```
  * 학습
  * ```python
    svc.fit(learn_data,learn_label) # fit은 학습을 시작
    ```
  * 출력 결과
  * ![svc fit(learn_data,learn_label)](https://github.com/Anjinhyoung/TIL-Today-I-Learned-/assets/117788976/b1ed1d4f-119d-45d5-8ad6-057d037e645e)
  * 검증 데이터 준비
  * ```python
    test_data = [[0,0],[1,0],[0,1],[1,1]]
    ```
  * 예측
  * ```python
    test_label = svc.predict(test_data)
    ```
  * 출력 결과
  * ```python
    array([0, 0, 0, 1])
    ```
  * 결과 검증
  * ```python
    print(test_data, '의 예측 결과:',test_label)
    ```
  * 출력 결과
  * ```python
    [[0, 0], [1, 0], [0, 1], [1, 1]] 의 예측 결과: [0 0 0 1]
    ```
  * 정답률 확인
  * ```python
    print('정답률:', accuracy_score([0,0,0,1],test_label))
    ```
  * 출력 결과
  * ```python
    정답률: 1.0
    ```
# 6. Iris DataSet
  * 데이터셋: 특정한 작업을 위해 데이터를 관련성 있게 모아놓은 것
  * [사이킷런 데이터셋 페이지](https://scikit-learn.org/stable/modules/classes.html?highlight=datasets#module-sklearn.datasets)
  * 사이트 들어가면 아래 사진처럼 여러 가지 DataSet이 있는데 그 중에서 Iris DataSet을 이용할 거다.
  * ![iris](https://github.com/Anjinhyoung/TIL-Today-I-Learned-/assets/117788976/a548df70-0ec1-4e72-9d09-0b3025b95c08)
  * irisDataSet 설명
  * ![irisDataSet](https://github.com/Anjinhyoung/TIL-Today-I-Learned-/assets/117788976/6a4d182f-2e23-403b-bf43-b7c91407cea7)
  * Classes: 꽃 종류(3가지)
  * Samples per class:각 종류마다 50개가 들어있다. 총 150개 꽃이 들어 있다.
  * Dimensionality: 독립변수(4가지가 있다.)
  * Features:말 그대로 꽃의 특징을 말한다.
  * [IRIS DataSet 활용](./IRIS_DataSet.ipynb)
# 7. Titanic DataSet
  * [Titanic DataSet Downloads](https://bit.ly/fc-ml-titanic)
  * 이 데이터셋은 타이타닉 호의 승객들에 대한 정보를 포함하고 있으며, 선실 등급, 성별, 나이 등 다양한 변수를 담고 있다.
  * [Titanic DataSet 활용](./Titanic_DataSet.ipynb)
# 8. 선형회귀
  * 데이터를 통해 가장 잘 설명할 수 있는 직선으로 데이터를 분석하는 방법
    - 단순 선형 회귀분석(단일 독립변수를 이용)
    - 다중 선형 회귀분석(다중 독립변수를 이용)
  * 우선 본격적으로 선형회귀를 하기 전 어떤 데이터를 가지고 사용하는지는 또 그 데이터를 어떻게 전처리를 하는지 링크를 걸어 둘 테니 코드를 보면서 참고하자
  * [Linear_Regression-1](./Linear_Regression-1.ipynb)
  * 'Linear_Regression-1' 코드를 봤으면  Linear_Regression-2를 보기 전에 선형회귀에 대해 좀 더 자세히 알아보자
      ### 8-1. 선형회귀 그리는 방법
         1. 산점도 그리기: 산점도는 데이터 포인트들이 2차원 평면 상에 어떻게 분포하는지를 나타내는 점들이라 생각하면 된다.
            x축은 입력 변수(독립 변수)를, y축은 출력 변수(종속 변수)를 나타낸다.

         2. 직선 그리기: 선형 회귀에서 찾아야 할 것은 데이터에 가장 적합한 '직선' 
            이 직선은 y = wx + b와 같은 형태를 가지며, w는 가중치(weight) b는 편향(bias)이라고 지칭
            처음에 직선을 그릴 때는 완전 정확하게는 아니지만 어느정도 비슷하게 데이터 포인트들의 위치에 따라서 직선을 그린다.
    
       a. ![선형 회귀-001](https://github.com/Anjinhyoung/TIL-Today-I-Learned-/assets/117788976/55b3d624-8bb7-49e5-a1fd-ff4a8fd259f8)

         2-1. 주황색 점들은 '실제값', 가운데 직선은 '기대값'
    
         3. 실제값과 기대값의 차이를 각각 다 더하기, 그런다음 개수만큼 나눠서 평균을 구하면 그 값은 'Loss'값이다.
            'Loss'값은 '기대값'과 '실제값'이 얼마나 틀렸는지 알려주는 값이다.
    
         3-1. 다만 위에 그림처럼 데이터 포인트 2개가 '기대값'보다 높고 나머지 2개는 '기대값'보다 낮아서 차이를 각각 더하면 상쇄된다.

       b. ![선형 회귀_2-001](https://github.com/Anjinhyoung/TIL-Today-I-Learned-/assets/117788976/13569cf9-5b64-40c9-8c40-3ef99576d346)

         3-2. 따라서 각각의 실제값과 기대값의 차이를 제곱하고 다 더하고 개수만큼 나눠서 평균을 구하면 그 값이 진정한 'Loss'값이다.

         4. 'Loss'값의 그래프
    
       c. ![Loss값의-그래프-001](https://github.com/Anjinhyoung/TIL-Today-I-Learned-/assets/117788976/abcddcc3-8b18-445a-bd6d-63ef3647f261)

         4-1. 현재 'Loss'값이 최소 'Loss'값으로 찾아 나서는 것이 'b'사진에 실제값과 기대값 차이가 안 나도록 기대값(직선)을 조절하는 것이다.
    
         4-2. 그러면 최소 'Loss'값을 찾아 나서는 방법은 'y = wx + b'에서 '가중치'랑 '편향'을 조절하면 된다. 즉 순간 기울기를 그리면 된다.
    
       d. ![Loss값의-그래프_2-001](https://github.com/Anjinhyoung/TIL-Today-I-Learned-/assets/117788976/4dfc05b7-0412-4f56-9cb0-22f0356b5730)

         4-3. 순간 기울기의 뻗어 나가는 반대 방향 쪽으로 값을 조절하면 최소 'Loss'값으로 갈 수 있다.  (순간 기울기는 편미분을 통해 구해진다.)
    
         4-4. 하지만 'y = wx + b' 이 식을 함부로 조절하게 되면 순간 기울기는 이상하게 그려질 수 있어서 최소 'Loss'값을 구하기 힘들어 질 수 있다.
    
         4-5. 따라서  y = wx + b * 0.00000a 같이 굉장히 작은 소수점을 곱해서 조금씩  순간 기울기를 조절할 수 있다. 0.00000a 같은 굉장히 작은 소수점을 러닝 메이트(학습률)이라고 한다.

         4-6. 이렇게 y = wx + b * 0.00000a 식을 통해 순간 기울기를 조절하는 알고리즘을 '그래디언트(gradient)'라고 한다.

         5. 지금까지는 단순 선형 회귀분석 설명이였고 다중 선형 회귀 분석은 기울기가 여러 개 있다고 생각을 하면 된다.

         5-1. 단순 선형 회귀분석의 식은 'y = wx + b'였는데 다중 선형 회귀 분석의 식은 'y = w1x1 + w2x2 + w3x3 .... + b'이다.
    
 * 선형회귀에 대해 이제 어느정도 이해했으면 선형회귀 코드를 한 번 보자.
 * [Linear_Regression-2](./Linear_Regression-2.ipynb)

# 9. 의사 결정 나무(Decision Tree)
  * 먼저 데이터 전처리 과정부터 보고오자
  * [Decision_Tree-1](./Decision_Tree_1.ipynb)
  * 전처리 과정을 다 봤으면 본격적으로 의사 결정 나무 알고리즘을 적용해보자.
    ### 의사 결정 나무(Decision Tree)
        * 데이터를 분류하거나 예측하기 위한 간단하고 직관적인 머신러닝 알고리즘
        * 지니계수 (Gini index): 불순도를 측정하는 지표
        * 엔트로피(Entropy): 불확실성 혹은 무질서의 정도를 측정하는 지표
                * 오버피팅(과적합): 학습데이터에서는 정확하나 테스트 데이터에서는 성과가 나쁜 현상을 말함 의사 결정 나무는 오버피팅이 매우 잘 일어남
                * 오버피팅을 방지하는 방법
                   - 사전 가지치기: 나무가 다 자라기 전에 알고리즘을 멈추는 방법
                   - 사후 가지치기: 나무를 끝까지 돌린 후 밑에서 부터 가지를 쳐 나가는 방법
  * [Decision_Tree-2](./Decision_Tree_2.ipynb)
  * 추가 설명
  * ```python
    plt.figure(figsize=(24, 12))
    plot_tree(dt, max_depth=5, fontsize=12, feature_names=X_train.columns)
    plt.show()
    ```
    ![추가 설명 사진](https://github.com/Anjinhyoung/TIL-Today-I-Learned/assets/117788976/a9a49cc3-f40e-4f0f-95af-5102501c860a)
    ```python
    max_depth=5 # 높이 5까지니깐 그 이상 부터는 생략으로 표시
    루트 노드 day_night_night기준으로 반씩 계속 나눈다.(day_night_night(24시간 중 21시 이상)가 독립 변수에 가장 영향력이 크다.)
    그 다음 독립 변수에 영향력이 큰 종속 변수는 temp_min: 최저온도, temp_max: 최고온도
    이런 식 계속 해서 독립 변수에 영향력이 큰 순서대로 종속 변수를 나열한다.(높이는 5까지)
 # 10. 로지스틱 회귀(Logistic Regression)
  * 데이터 전처리 과정
  * [Logistic_Regression-1](./Logistic_Regression_1.ipynb)
  * 전처리 과정을 다 봤으면 본격적으로 로지스틱 회귀를 적용해보자
    ### 로지스틱 회귀(Logistic Regression)
        * 둘 중의 하나를 결정하는 문제(이진 분류)를 풀기 위한 대표적인 알고리즘
        * 입력 데이터와 가중치의 선형 조합으로 선형 방적식을 만듦 -> 선형 방정식의 결과를 0과 1사이의 확률값으로 변환(시그모이드 함수)
        * 임계값:분류 작업에서 예측된 확률값을 이용하여 레이블(정답)을 결정하는 기준 값
        * 3개 이상의 클래스에 대한 판별을 할 수 있음
              * OvR(One -vs-Rest):각 클래스마다 하나의 이진 분류기를 만들고, 해당 클래스를 기준으로 그 클래스와 모든 클래스를 구분하는 이진 분류를 수행  -> 가장 높은 확률을 가진 클래스를 선택
              * OvO(One-vs-One): 클래스 개수가 N인 경우 N(N-1)2개의 이진 분류기를 만듦
                -> 각 이진 분류기는 두 개의 클래스만 구분하는데, 해당 클래스와 나머지 클래스 간에 이진 분류를 수행 -> 입력 데이터를 각 이진 분류기에 넣어 가장 많이 선태된 클래스를 최종 선택

 # 11. SVM(Suppert Vector Machine)
  * 두 클래스로부터 최대한 멀리 떨어져 있는 결정 경계를 찾는 분류기로 특정 조건을 만족하는 동시에 클래스를 분류하는 것을 목표로 함(분류, 예측 둘 다 가능함)

  ![추가 설명 사진](https://github.com/Anjinhyoung/Coding-Learning/assets/117788976/cce13933-9236-40d2-873a-f067516869b6)

  * [SVM 활용하기 전 데이터 전처리, SVM 활용하기](./SVM.ipynb)

 # 12. 랜덤포레스트(Random Forest)
  * 머신러닝에서 많이 사용되는 앙상블 기법 중 하나이면서 결정 트리(Decison Tree)를 기반으로 함
  * 학습을 통해 구성해 놓은 결정 트리로부터 분류 결과를 취합해서 결론을 얻는 방식
  * 성능은 꽤 우수한 편이나 오버피팅 하는 경향이 있음

  * [랜덤포레스트 활용하기 전 데이터 전처리, 랜덤포레스트 활용하기](./Random_forest.ipynb)

 # 13. LGBM(lightGBM)
  * 마이크로소프트에서 개발한 Gradient Boostring FrameWork
  * 리프 중심 히스토그램 기반 알고리즘
       * 트리를 균형적으로 분할하는 것이 아니라, 최대한 불균형하게 분할한다.
       * 특성들의 분포를 히스토그램으로 나타내고, 해당 히스토그램을 이용하여 빠르게 후보 분할 기준을 선택
       * 후보 분할 기준 중에서 최적의 분할 기준을 선택하기 위해, 데이터 포인트들을 히스토그램에 올바르게 배치하고, 이용하여 최적의 분할 기준을 선택
  * GBM(Gradient Boostring FrameWork): 모델1을 통해 y를 예측하고, 모델2에 데이터를 넣어 y를 예측, 모델3에 넣어 y를 예측하는 방식
  * 작은 데이터셋에서도 높은 성능을 보이며, 특히 대용량 데이터셋에서 다른 Gradient Boostring  알고리즘보다 빠르게 학습
  * 메모리 사용량이 상대적으로 적은편
  * 적은 데이터셋을 사용할 경우 과적합 가능성이 매우 큼(일반적으로 데이터가 10,000개 이상은 사용해야 함)
  * 조기중단(early stopping)을 지원
  * 약한 학습기들을 순차적으로 학습시켜 강력한 학습기를 만듦
  * 약한 학습기(단일 모댈)
  * 이전 오차를 보완해가면서 가중치를 부여

  * [LGBM 활용하기 전 데이터 전처리, LGBM 활용하기](./lightGBM.ipynb)

 # 14. 다양한 모델 적용하기(지도 학습으로만)
  * [다양한 모델 활용하기](./다양한_모델_적용.ipynb)
